# Where to store checkpoints & results
model_save_path  = "ckpt_finetune"
result_save_path = "sample_results"

[data]
dequant_dist       = "none"
int_dequant_factor = 0

[unimodmlp_params]
num_layers = 2
d_token    = 4
n_head     = 1
factor     = 32
bias       = true
dim_t      = 1024
use_mlp    = true
d_numerical = 65
# ... your category sizes ...
categories = [30,5,4,3,3,10,3,3,13,15,57,8,7,5,5,5,92,12,7,22]

[diffusion_params]
# Increase from the default of 4 → 50 for higher fidelity
num_timesteps   = 50
scheduler       = "power_mean_per_column"
cat_scheduler   = "log_linear_per_column"
noise_dist      = "uniform_t"

[diffusion_params.sampler_params]
stochastic_sampler       = true
second_order_correction  = true

[diffusion_params.edm_params]
precond            = true
sigma_data         = 1.0
net_conditioning   = "sigma"

[diffusion_params.noise_dist_params]
P_mean = -1.2
P_std  = 1.2

[diffusion_params.noise_schedule_params]
sigma_min   = 0.002
sigma_max   = 80
rho         = 7
eps_max     = 0.001
eps_min     = 1e-05
rho_init    = 7.0
rho_offset  = 5.0
k_init      = -6.0
k_offset    = 1.0

[train.main]
# ← Seed/pretrain or whatever you pass via --pre-steps
steps             = 10
batch_size        = 2048
lr                = 1e-3
# never run interim sampling until after all epochs
check_val_every   = 9999

lr_scheduler       = "reduce_lr_on_plateau"
factor             = 0.9
reduce_lr_patience = 50

closs_weight_schedule = "anneal"
c_lambda              = 1.0
d_lambda              = 1.0

[sample]
batch_size = 10000

# deterministic eval
deterministic = false
